<h1 id="homework1columbiaeecs6894httpscolumbia6894githubio">Homework 1, <a href="https://columbia6894.github.io/">Columbia EECS 6894</a></h1>

<p>Note: all the programming tasks must be finished with <a href="http://jupyter.org/">Jupyter notebook</a></p>

<h2 id="problem1">Problem 1</h2>

<p>Tensorflow can compute the gradient automatically. The following code snippet shows an example of
computing the gradient for mean square loss.</p>

<pre><code class="py language-py">import tensorflow as tf
label = tf.constant(1.0, dtype=tf.float32)
x = tf.placeholder(tf.float32)

loss_mse = tf.losses.mean_squared_error(label, x)
gradient_mse = tf.gradients(loss_mse, x)

with tf.Session() as sess:
    ci, gi = sess.run((loss_mse, gradient_mse), feed_dict={x: 1.0})
    print 'mse, loss, grad = ', ci, gi
</code></pre>

<p>Please extend the above code to compute:</p>

<ol>
<li>The gradient of hinge loss when label = 1.0, x = 1.001</li>

<li>The gradient of hinge loss when label = 1.0, x = 0.009</li>

<li>Plot the curves of gradients and losses for x in [-2, 2]  (hint: use <code>%matplotlib inline</code> in Jypiter)</li>
</ol>

<h2 id="problem2">Problem 2</h2>

<p>Logistic regression and multi-layer perceptrons (MLPs) are two basic models for classification tasks.
Try to use these two models to learn from the following xor dataset:</p>

<pre><code>import numpy
xs = np.array([[-1.1, 1.0], [-1.0, 1.1], [-1.1, 1.1], [1.0, -1.1],[1.1, -1.0],[1.0, -1.0],
                  [1.1, 1.1],[1.0, 0.9],[1.1, 1.0],  [-1.1, -1.0], [-1.1, -1.1], [-1.0, -1.1]],
                dtype=np.float32)
ys = np.array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], dtype=np.float32)
ys = ys[:, None]
</code></pre>

<p>Train and Evaluate your models using <code>xs</code> (samples) and <code>ys</code> (training labels). You are allowed to use
<a href="https://keras.io/">Keras</a>. Compare the performance of your two models.</p>

<h2 id="problem3">Problem 3</h2>

<p><a href="http://yann.lecun.com/exdb/mnist/">The MNIST dataset of handwritten digits</a> is a very popular dataset to
test the algorithms and ideas of machine learning. To train MNIST data, the following procedures are adopted:</p>

<ul>
<li>Reshape the digit pictures ( each with 28x28 pixels) to vectors of 784</li>

<li>Change the type of xs to float32</li>

<li>Every pixel is from 0 to 255. Renormalize it to 0 and 1</li>

<li>Reshape the label vectors ys if necessary</li>
</ul>

<p>The original MNIST data has 10 categories. Our new task is to take only two categories: digit 4 and digit 8 and
train a classifier. You are suggested to compare two models:</p>

<ol>
<li>One hidden layer MLP with cross entropy loss</li>

<li>One hidden layer MLP with hinge loss</li>

<li>(bonus) MLP with two and three hidden layers
</ol>

<p><em>hint: you may refer to <a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py">Keras example</a></em>.</p>